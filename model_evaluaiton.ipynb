{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow_from_directory\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.applications.resnet import ResNet152\n",
    "from tensorflow.keras.regularizers import L2\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "\n",
    "# Test data\n",
    "test_generator = ImageDataGenerator(rescale=1./255)\n",
    "test = test_generator.flow_from_directory('data/cropped/mass/test/', target_size=(IMAGE_SIZE, IMAGE_SIZE), batch_size=16, class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create roc curve\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict(test)\n",
    "\n",
    "# Get the true values from the generator\n",
    "true_values = test.classes\n",
    "\n",
    "# Get the predicted probabilities for each class\n",
    "pred_probabilities = predictions\n",
    "\n",
    "# Get the false positive rate and true positive rate\n",
    "fpr, tpr, thresholds = roc_curve(true_values, pred_probabilities)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label='ResNet152')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall, percission and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper funcitons\n",
    "def get_recall_i(conf_mat, i):\n",
    "    \"\"\"\n",
    "    Computes.\n",
    "    \"\"\"\n",
    "    return conf_mat[i, i] / np.sum(conf_mat[:, i])\n",
    "\n",
    "\n",
    "def get_all_recalls(conf_mat):\n",
    "    \"\"\"\n",
    "    Computes All recalls\n",
    "    \"\"\"\n",
    "\n",
    "    return [get_recall_i(conf_mat, i) for i in range(conf_mat.shape[0])]\n",
    "\n",
    "\n",
    "def get_percision_i(conf_mat, i):\n",
    "    \"\"\"\n",
    "    Computes.\n",
    "    \"\"\"\n",
    "    return conf_mat[i, i] / np.sum(conf_mat[i, :])\n",
    "\n",
    "\n",
    "def get_all_percisions(conf_mat):\n",
    "    \"\"\"\n",
    "    Computes all percisions.\n",
    "    \"\"\"\n",
    "\n",
    "    return [get_percision_i(conf_mat, i) for i in range(conf_mat.shape[0])]\n",
    "\n",
    "def get_f1_scores(conf_mat):\n",
    "    \"\"\"\n",
    "    Computes the f1 scores of the given confusion matrix.\n",
    "    \"\"\"\n",
    "    recalls = get_all_recalls(conf_mat)\n",
    "    percisions = get_all_percisions(conf_mat)\n",
    "    f1_scores = []\n",
    "    for i in range(len(recalls)):\n",
    "        f1_scores.append(2 * (percisions[i] * recalls[i]) / (percisions[i] + recalls[i]))\n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "def get_overall_accuracy(conf_mat):\n",
    "    \"\"\"\n",
    "    Computes the overall accuracy of the given confusion matrix.\n",
    "    \"\"\"\n",
    "    return np.trace(conf_mat) / np.sum(conf_mat)\n",
    "\n",
    "def get_confusion_matrix(result):\n",
    "    result[\"Rounded\"] = result[\"Prediction\"].round()\n",
    "    \n",
    "    # creates confusion matrix\n",
    "    true_positive_fake = result[(result['Filename'].str.contains(\n",
    "        'benign')) & (result['Rounded'] == 0)].count()[0]\n",
    "    false_positive_fake = result[(result['Filename'].str.contains(\n",
    "        'malignant')) & (result['Rounded'] == 0)].count()[0]\n",
    "\n",
    "    true_positive_real = result[(result['Filename'].str.contains(\n",
    "        'malignant')) & (result['Rounded'] == 1)].count()[0]\n",
    "    false_positive_real = result[(result['Filename'].str.contains(\n",
    "        'benign')) & (result['Rounded'] == 1)].count()[0]\n",
    "\n",
    "    conf_matrix = np.matrix([\n",
    "        [true_positive_fake, false_positive_fake],\n",
    "        [false_positive_real, true_positive_real]\n",
    "    ])\n",
    "    \n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions and evaluation\n",
    "pred_res = model.predict(\n",
    "            test,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = pd.DataFrame({\n",
    "                \"Filename\": test.filenames,\n",
    "                \"Prediction\": pred_res.flatten()\n",
    "            })\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = get_confusion_matrix(test_results)\n",
    "print(\"conf_matrix: \\n\", conf_matrix)\n",
    "print(\"Recalls:\", np.round(get_all_recalls(conf_matrix), 3))\n",
    "print(\"Precisions:\", np.round(get_all_percisions(conf_matrix), 3))\n",
    "print(\"F1 Scores:\", np.round(get_f1_scores(conf_matrix), 3))\n",
    "print(\"Accurcy:\", np.round(get_overall_accuracy(conf_matrix), 3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
